{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkkfpsadtTXD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import os\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class MinesweeperEnv(gym.Env):\n",
        "    def __init__(self, board_size=9, num_mines=10):\n",
        "        super(MinesweeperEnv, self).__init__()\n",
        "\n",
        "        self.board_size = board_size\n",
        "        self.num_mines = num_mines\n",
        "\n",
        "        self.action_space = spaces.Discrete(board_size * board_size)\n",
        "        self.observation_space = spaces.Box(low=0, high=2, shape=(2, board_size, board_size), dtype=np.int64)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
        "        self.state = np.zeros((2, self.board_size, self.board_size), dtype=int)\n",
        "\n",
        "        self.mines = np.random.choice(self.board_size * self.board_size, self.num_mines, replace=False)\n",
        "        for mine in self.mines:\n",
        "            x, y = divmod(mine, self.board_size)\n",
        "            self.board[x, y] = -1\n",
        "\n",
        "        for i in range(self.board_size):\n",
        "            for j in range(self.board_size):\n",
        "                if self.board[i, j] == -1:\n",
        "                    continue\n",
        "                count = 0\n",
        "                for x in range(max(0, i - 1), min(self.board_size, i + 2)):\n",
        "                    for y in range(max(0, j - 1), min(self.board_size, j + 2)):\n",
        "                        if self.board[x, y] == -1:\n",
        "                            count += 1\n",
        "                self.board[i, j] = count\n",
        "\n",
        "        self.done = False\n",
        "        self.steps = 0\n",
        "        self.total_reward = 0\n",
        "        self.mine_hit = False\n",
        "        self.first_click = True\n",
        "\n",
        "        return self._get_observation()\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = divmod(action, self.board_size)\n",
        "        if self.first_click:\n",
        "            if self.board[x, y] == -1:\n",
        "                self._relocate_mine(x, y)\n",
        "            self.first_click = False\n",
        "\n",
        "        if self.state[0, x, y] == 1:\n",
        "            reward = -1\n",
        "            done = False\n",
        "        elif self.board[x, y] == -1:\n",
        "            self.state[0, x, y] = 1\n",
        "            reward = -10\n",
        "            self.mine_hit = True\n",
        "            done = True\n",
        "        else:\n",
        "            self.reveal_cells(x, y)\n",
        "            reward = 1\n",
        "            done = self.check_done()\n",
        "\n",
        "            if done and not self.mine_hit:\n",
        "                reward = 10\n",
        "\n",
        "        self.total_reward += reward\n",
        "        self.steps += 1\n",
        "\n",
        "        return self._get_observation(), reward, done, {}\n",
        "\n",
        "    def _relocate_mine(self, x, y):\n",
        "        self.board[x, y] = 0\n",
        "        possible_positions = set(range(self.board_size * self.board_size)) - set(self.mines)\n",
        "        if x * self.board_size + y in possible_positions:\n",
        "            possible_positions.remove(x * self.board_size + y)\n",
        "        new_mine_position = np.random.choice(list(possible_positions))\n",
        "        new_x, new_y = divmod(new_mine_position, self.board_size)\n",
        "        self.board[new_x, new_y] = -1\n",
        "        self.mines = [m for m in self.mines if m != x * self.board_size + y]\n",
        "        self.mines.append(new_mine_position)\n",
        "        self._update_board_counts()\n",
        "\n",
        "    def _update_board_counts(self):\n",
        "        for i in range(self.board_size):\n",
        "            for j in range(self.board_size):\n",
        "                if self.board[i, j] == -1:\n",
        "                    continue\n",
        "                count = 0\n",
        "                for x in range(max(0, i - 1), min(self.board_size, i + 2)):\n",
        "                    for y in range(max(0, j - 1), min(self.board_size, j + 2)):\n",
        "                        if self.board[x, y] == -1:\n",
        "                            count += 1\n",
        "                self.board[i, j] = count\n",
        "\n",
        "    def reveal_cells(self, x, y):\n",
        "        stack = [(x, y)]\n",
        "        while stack:\n",
        "            cx, cy = stack.pop()\n",
        "            if cx < 0 or cx >= self.board_size or cy < 0 or cy >= self.board_size:\n",
        "                continue\n",
        "            if self.state[0, cx, cy] == 1:\n",
        "                continue\n",
        "\n",
        "            self.state[0, cx, cy] = 1\n",
        "            self.state[1, cx, cy] = self.board[cx, cy]\n",
        "\n",
        "            if self.board[cx, cy] == 0:\n",
        "                for dx in range(-1, 2):\n",
        "                    for dy in range(-1, 2):\n",
        "                        if dx != 0 or dy != 0:\n",
        "                            stack.append((cx + dx, cy + dy))\n",
        "\n",
        "    def _get_observation(self):\n",
        "        norm_state = self.state.astype(np.float32)\n",
        "        norm_state[1, :, :] = norm_state[1, :, :] / 8.0\n",
        "        return norm_state\n",
        "\n",
        "    def check_done(self):\n",
        "        unopened_cells = np.sum(self.state[0, :, :] == 0)\n",
        "        if unopened_cells == self.num_mines:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def render(self, state=None):\n",
        "        if state is None:\n",
        "            state = self.state\n",
        "\n",
        "        render_state = np.full(shape=(self.board_size, self.board_size), fill_value=\".\")\n",
        "\n",
        "        for i in range(self.board_size):\n",
        "            for j in range(self.board_size):\n",
        "                if state[0, i, j] == 0:\n",
        "                    render_state[i, j] = \".\"\n",
        "                elif self.board[i, j] == -1:\n",
        "                    render_state[i, j] = \"M\"\n",
        "                else:\n",
        "                    render_state[i, j] = str(self.board[i, j])\n",
        "\n",
        "        render_state = pd.DataFrame(render_state)\n",
        "        render_state = render_state.style.applymap(self.render_color)\n",
        "        display(render_state)\n",
        "\n",
        "    def render_answer(self):\n",
        "        render_state = np.full(shape=(self.board_size, self.board_size), fill_value=\".\")\n",
        "\n",
        "        for i in range(self.board_size):\n",
        "            for j in range(self.board_size):\n",
        "                if self.board[i, j] == -1:\n",
        "                    render_state[i, j] = \"M\"\n",
        "                else:\n",
        "                    render_state[i, j] = str(self.board[i, j])\n",
        "\n",
        "        render_state = pd.DataFrame(render_state)\n",
        "        render_state = render_state.style.applymap(self.render_color)\n",
        "        display(render_state)\n",
        "\n",
        "    def render_color(self, var):\n",
        "        color = {\n",
        "            '0': 'black', '1': \"skyblue\", '2': 'lightgreen', '3': 'red', '4': 'violet',\n",
        "            '5': 'brown', '6': 'turquoise', '7': 'grey', '8': 'black', 'M': 'white', '.': 'black'\n",
        "        }\n",
        "        return f\"color: {color[var]}\"\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=(3, 3), stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        conv_output_size = self._get_conv_output(state_shape)\n",
        "        self.fc1 = nn.Linear(conv_output_size, 256)\n",
        "        self.fc2 = nn.Linear(256, action_size)\n",
        "\n",
        "    def _get_conv_output(self, shape):\n",
        "        input = torch.rand(1, *shape)\n",
        "        output = self.pool1(torch.relu(self.conv1(input)))\n",
        "        output = self.pool2(torch.relu(self.conv2(output)))\n",
        "        return int(np.prod(output.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "MEM_SIZE_MAX = 50000\n",
        "MEM_SIZE_MIN = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "DISCOUNT = 0.1\n",
        "\n",
        "EPSILON = 1.0\n",
        "EPSILON_DECAY = 0.995\n",
        "EPSILON_MIN = 0.01\n",
        "\n",
        "UPDATE_TARGET_EVERY = 5\n",
        "EPISODES = 10000\n",
        "MAX_STEPS = 71\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_size):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=MEM_SIZE_MAX)\n",
        "\n",
        "        self.model = CNN(state_shape, action_size).cuda()\n",
        "        self.target_model = CNN(state_shape, action_size).cuda()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.epsilon = EPSILON\n",
        "        self.losses = []\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon *= EPSILON_DECAY\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        state = self.normalize_state(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).cuda()\n",
        "        with torch.no_grad():\n",
        "            act_values = self.model(state)\n",
        "        return torch.argmax(act_values).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < MEM_SIZE_MIN:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
        "\n",
        "        states = torch.FloatTensor([self.normalize_state(m[0]) for m in minibatch]).cuda()\n",
        "        actions = torch.LongTensor([m[1] for m in minibatch]).cuda()\n",
        "        rewards = torch.FloatTensor([m[2] for m in minibatch]).cuda()\n",
        "        next_states = torch.FloatTensor([self.normalize_state(m[3]) for m in minibatch]).cuda()\n",
        "        dones = torch.FloatTensor([m[4] for m in minibatch]).cuda()\n",
        "\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.target_model(next_states).max(1)[0]\n",
        "        target_q_values = rewards + (DISCOUNT * next_q_values * (1 - dones))\n",
        "\n",
        "        loss = self.loss_fn(q_values, target_q_values.detach())\n",
        "        self.losses.append(loss.item())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def normalize_state(self, state):\n",
        "        norm_state = state.astype(np.float32)\n",
        "        norm_state[1, :, :] = norm_state[1, :, :] / 8.0\n",
        "        return norm_state\n",
        "\n",
        "def plot_metrics(episode_list, avg_rewards, avg_steps, success_rates, avg_loss):\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(episode_list, avg_rewards, label='Average Reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Average Reward over Episodes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(episode_list, avg_steps, label='Average Steps', color='orange')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Steps')\n",
        "    plt.title('Average Steps over Episodes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(episode_list, success_rates, label='Success Rate', color='green')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Success Rate (%)')\n",
        "    plt.title('Success Rate over Episodes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(episode_list, avg_loss, label='Average Loss', color='red')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Loss')\n",
        "    plt.title('Average Loss over Episodes')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "env = MinesweeperEnv()\n",
        "state_shape = env.observation_space.shape\n",
        "action_size = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_shape, action_size)\n",
        "\n",
        "episode_rewards = []\n",
        "total_steps = []\n",
        "success_rates = []\n",
        "episode_list = []\n",
        "losses = []\n",
        "avg_rewards = []\n",
        "avg_steps = []\n",
        "avg_loss = []\n",
        "\n",
        "success_count = 0\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < MAX_STEPS:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        agent.replay()\n",
        "        steps += 1\n",
        "\n",
        "    agent.update_epsilon()\n",
        "    total_steps.append(steps)\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    if episode % UPDATE_TARGET_EVERY == 0:\n",
        "        agent.update_target_model()\n",
        "\n",
        "    if done and (reward == 10 or not env.mine_hit):\n",
        "        success_count += 1\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward_last_100 = np.mean(episode_rewards[-100:])\n",
        "        avg_steps_last_100 = np.mean(total_steps[-100:])\n",
        "        success_rate = (success_count / (episode + 1)) * 100\n",
        "\n",
        "        avg_rewards.append(avg_reward_last_100)\n",
        "        avg_steps.append(avg_steps_last_100)\n",
        "        success_rates.append(success_rate)\n",
        "        episode_list.append(episode + 1)\n",
        "\n",
        "        avg_loss_last_100 = np.mean(agent.losses[-100:])\n",
        "        avg_loss.append(avg_loss_last_100)\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Average Reward : {avg_reward_last_100:.2f}, Average Steps: {avg_steps_last_100:.2f}, Success Rate: {success_rate:.2f}%, Avg Loss: {avg_loss_last_100:.4f}, Epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "plot_metrics(episode_list, avg_rewards, avg_steps, success_rates, avg_loss)\n",
        "\n",
        "agent.epsilon = 0.0\n",
        "success_count = 0\n",
        "\n",
        "episodes = 1000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < MAX_STEPS:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    success = done and (reward == 10 or not env.mine_hit)\n",
        "    if success:\n",
        "        success_count += 1\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Success: {success}, Steps: {steps}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "success_rate = (success_count / episodes) * 100\n",
        "print(f\"Success Rate: {success_rate:.2f}%\")\n"
      ]
    }
  ]
}